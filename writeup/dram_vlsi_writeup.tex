\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{graphicx} 

\title{16$\times$4 DRAM Project Report}
\author{Adithya Selvakumar \& Krishna Karthikeya Chemudupati}
\date{}

\begin{document}
\maketitle

\newpage

\tableofcontents
\newpage

% -----------------------------------------
\section{Memory Description}

\subsection{Text Description of Memory Operation}
% 1.1

The proposed memory is a 16$\times$4 dynamic random-access array in which each bit is stored as charge on a dedicated capacitor accessed through a single NMOS switch. Because the stored information is represented by an analog voltage rather than a bistable digital state, every memory operation must explicitly manage the charge on the storage capacitor and the large distributed capacitance of the bitline. At the system level, the memory supports synchronous read and write access to any of the sixteen 4-bit words using a shared periphery consisting of a row decoder, bitline precharge network, write drivers, and dynamic sense amplifiers. All operations follow a well-defined sequence controlled by the global clock to ensure that the bitline reference level, access timing, and sense-amplifier activation are coordinated precisely.

Before any access, each bitline is precharged to a known reference voltage, typically $V_{\mathrm{DD}}/2$, to establish a balanced initial condition for charge sharing. This reference level minimizes read disturb, reduces voltage swing requirements, and ensures that both a ``stored zero'' and a ``stored one'' shift the bitline in opposite directions by comparable magnitudes. Once precharge is complete, the selected wordline is asserted. This connects each storage capacitor in the addressed row to its respective bitline, initiating a small charge redistribution governed by the ratio between the cell capacitance $C_{S}$ and the total bitline capacitance $C_{\mathrm{BL}}$. The resulting voltage perturbation,
\[
\Delta V = \frac{C_{S}}{C_{S}+C_{\mathrm{BL}}} \left( V_{\mathrm{cell}} - V_{\mathrm{PRE}} \right),
\]
is typically only a few hundred millivolts, and therefore insufficient to be interpreted directly as a logic level. A dynamic sense amplifier resolves this small deviation into a full-rail digital value by temporarily isolating the bitline and regeneratively amplifying the sign of the perturbation. This same regeneration step also rewrites the correct level back into the cell, ensuring that the destructive read is paired with immediate restoration.

Write operations bypass the charge-sharing mechanism by directly forcing full-swing logic levels onto the bitlines using dedicated write drivers. When the wordline is raised, the bitline voltage overwrites the charge on the storage capacitor through the access transistor. Because the NMOS access device imposes a threshold drop when writing a logical \texttt{1}, the write driver is sized and biased such that the cell capacitor is still charged to a level distinguishable from a stored \texttt{0} during subsequent reads. After the wordline returns low, the capacitor retains its updated voltage until it naturally leaks.

Due to leakage through the access transistor and junction parasitics, the stored charge decays over time, making periodic refreshing essential for correct operation. The refresh controller cycles through all sixteen rows, performing a controlled read-and-restore sequence identical to a normal read but without driving data externally. This ensures that each capacitor is periodically replenished before its voltage degrades below the sense margin. Together, these operations define a fully synchronous DRAM system in which precise timing of precharge, activation, sensing, and restoration enables reliable random access despite the analog and transient nature of charge-based storage.

\newpage

\subsection{Description of Timing Requirements}
% 1.2

Reliable operation of the DRAM array is governed by a sequence of tightly constrained timing phases that control how charge is prepared, transferred, sensed, and restored. Because the stored data is represented as charge on a small capacitor rather than a stable latch, each phase must guarantee proper settling of analog voltages before the next action occurs. The minimum allowable clock period for the memory is thus defined by the slowest of these phases, and any deviation in their ordering or duration risks incorrect sensing, incomplete restoration, or accelerated data loss.

The timing sequence begins with bitline precharge, during which all columns are driven to a uniform reference level $V_{\mathrm{PRE}} = V_{\mathrm{DD}}/2$. This must fully accommodate the large bitline capacitance $C_{\mathrm{BL}}$, ensuring that every bitline settles within a narrow tolerance around the reference. Since $C_{\mathrm{BL}} \gg C_{S}$, insufficient precharge time directly reduces sense margin and increases the risk of ambiguous reads. Only once the bitlines reach steady state is the precharge network disabled, placing a firm lower bound on precharge duration dictated by the driver strength and bitline RC.

After precharge, the selected wordline is asserted. Wordline activation timing is critical because the access transistor connects the small cell capacitor to the large bitline, initiating charge sharing. The activation time must be long enough for the bitline perturbation
\[
\Delta V = \frac{C_{S}}{C_{S}+C_{\mathrm{BL}}} \left( V_{\mathrm{cell}} - V_{\mathrm{PRE}} \right)
\]
to reach a detectable level. If the wordline is deasserted too early, $\Delta V$ may fall within the sense amplifier’s metastability region; if held too long, leakage and parasitic conduction begin to counteract the perturbation. The required pulse width therefore depends on the access transistor’s on-resistance, wordline RC delay, and the cell-to-bitline capacitance ratio.

The sense amplifier enable (SE) must be carefully aligned after the charge-sharing interval. Activating the sense amplifier too early risks incorrect polarity resolution, while enabling it too late allows the bitline to drift back toward $V_{\mathrm{PRE}}$. Once SE is asserted, the amplifier regeneratively drives the bitline to a full-rail level and simultaneously restores the cell’s charge. The sense window must remain open long enough for complete regeneration, since partial restoration reduces retention time and can corrupt subsequent reads.

Write operations impose their own timing requirements. The write driver must establish the correct bitline level before wordline activation, and the wordline must remain high long enough for the cell capacitor to charge or discharge through the NMOS access device. Writing a logical ``1'' is particularly sensitive because of the $V_{\mathrm{Tn}}$ drop across the access transistor, requiring a settling interval sufficient to leave the capacitor with a distinguishable stored high level. Wordline deassertion must occur only after the capacitor reaches its intended voltage.

Finally, periodic refresh introduces a global timing constraint. Every row must be revisited within the allowed retention interval using a read-and-restore operation identical to a normal read but hidden from external interfaces. The refresh window is determined by leakage paths and the minimum resolvable $\Delta V$ of the sense amplifier. The clock must therefore allocate enough idle cycles to refresh all sixteen rows, establishing an upper bound on usable memory bandwidth.



\newpage

\subsection{Optimization and Design Tradeoffs}
% 1.3



\newpage

% -----------------------------------------
\section{Schematics with Size Annotations}
% 2

\subsection{Baseline Design}

In this section, we present the baseline DRAM-based 16$\times$4 memory design using schematic views from Cadence, annotated with device dimensions for all transistors. The baseline implementation prioritizes functional correctness and clear hierarchy over aggressive optimization of area, power, or delay. Each schematic is organized according to the design tiers defined in our architecture: primitive logic gates (Tier~0), core analog and memory blocks (Tier~1), row and column infrastructure (Tier~2), control logic (Tier~3), verification environment (Tier~4), and full-array integration (Tier~5). For each circuit, we will eventually include a figure reference and a brief description of sizing choices, focusing on transistor widths, lengths, and any deliberate upsizing for drive strength or noise margin.

\subsubsection{Tier 0: Logic Primitives}

\noindent\underline{INV} \\
% Include schematic of baseline inverter with W/L annotations for pull-up and pull-down devices.
% Discuss chosen ratio and its use as a reference cell for other gates.



\newpage

\noindent\underline{NAND2} \\
% Include schematic of 2-input NAND gate sized relative to INV.
% Note series NMOS stack sizing and any upsizing for critical paths (e.g., decoder).



\newpage

\noindent\underline{NOR2} \\
% Include schematic of 2-input NOR gate with size annotations.
% Comment on parallel NMOS / series PMOS sizing, especially where used in predecoding.



\newpage

\subsubsection{Tier 1: Core Memory and Analog Blocks}

\noindent\underline{1T1C Bitcell} \\
% Show 1T1C DRAM cell schematic with access NMOS W/L and explicit storage capacitor value.
% Note chosen CS and its relationship to CBL and sense margin.

For the baseline 1T1C DRAM implementation, the storage capacitance $C_{S}$ and bitline capacitance $C_{\mathrm{BL}}$ were chosen to ensure correct functional behavior while preserving the qualitative characteristics of dynamic charge-based memory operation. Since all transistors in the baseline design use the minimum geometry of 120\,nm/45\,nm, the access device presents a relatively high on-resistance, and the charge-sharing dynamics must be driven primarily by the relative magnitudes of the explicit capacitors. We selected a storage capacitor of $C_{S}=20\,\mathrm{fF}$, representative of a small integrated capacitor in deeply-scaled technologies, and a bitline capacitance of $C_{\mathrm{BL}}=80\,\mathrm{fF}$, approximately four times larger to reflect the fact that real bitlines accumulate diffusion, wiring, and access-transistor capacitances from many cells. With these values, the read perturbation generated during charge sharing is
\[
\Delta V = \frac{C_{S}}{C_{S}+C_{\mathrm{BL}}}\left(V_{\mathrm{cell}} - V_{\mathrm{PRE}}\right) \approx 0.2 \times 0.6\,\mathrm{V} = 120\,\mathrm{mV},
\]
which produces a clear, measurable deviation from the precharge level while remaining within the small-signal regime expected of DRAM sensing. This $\Delta V$ is sufficiently large to be resolved by the baseline dynamic sense amplifier without requiring aggressive device sizing or timing precision, yet small enough to preserve the qualitative behavior of realistic DRAM columns where $C_{\mathrm{BL}} \gg C_{S}$. These capacitor values also ensure that the RC time constant of the charge-sharing process remains manageable despite the minimum-sized access transistor, enabling full read and restore behavior within relaxed wordline pulse widths during functional verification. As a result, the chosen $C_{S}$ and $C_{\mathrm{BL}}$ provide a robust and physically meaningful baseline environment in which the full read, write, and refresh mechanisms can be validated prior to any optimization of area, power, or delay.

\newpage

\noindent\underline{Precharge Circuit} \\
% Show bitline precharge network that drives BL to VDD/2 (or chosen reference).
% Annotate precharge transistor sizes and any equalization devices between bitlines.



\newpage

\noindent\underline{Dynamic Sense Amplifier} \\
% Show dynamic sense amp schematic (cross-coupled structure, isolation devices, SE input).
% Annotate input pair and load device sizes; explain relative sizing for gain and speed.



\newpage

\noindent\underline{Write Driver} \\
% Show write driver that forces BL to logic 0/1 during writes.
% Include W/L for pull-up and pull-down devices and any series gating controlled by WE.



\newpage

\subsubsection{Tier 2: Row and Column Infrastructure}

\noindent\underline{Wordline Driver} \\
% Show buffer chain or driver inverter for WL, sized to drive total wordline capacitance.
% Annotate each stage and indicate fanout / tapering if used.



\newpage

\noindent\underline{Row Decoder (4-to-16)} \\
% Show hierarchical 4-to-16 decoder schematic using Tier 0 gates.
% Annotate transistor sizes for predecode and final wordline-select stages.



\newpage

\subsubsection{Tier 3: Control Logic}

\noindent\underline{Read/Write FSM} \\
% Show state machine or control logic that sequences precharge, activate, sense, and write.
% Note that logic is built from Tier 0 gates; annotate key transistor sizes where timing-critical.



\newpage

\noindent\underline{Refresh FSM} \\
% Show refresh controller that iterates through row addresses within retention interval.
% Include any counters or control outputs with gate-level implementation.



\newpage

\noindent\underline{Precharge Control} \\
% Show gating logic that generates precharge enable and equalization signals.
% Annotate devices driving precharge transistors and their sizing.



\newpage

\subsubsection{Tier 4: Verification and Test Infrastructure}

\noindent\underline{Testbench} \\
% Top-level schematic used only for simulation, instantiating the DRAM module and sources.
% Include controlled voltage/current sources and measurement points.



\newpage

\noindent\underline{Stimulus} \\
% Show pattern-generation logic or piecewise sources for address, data, WE, and control signals.
% Document any parameterized sources used for read/write sequences and stress tests.



\newpage

\noindent\underline{CLK Block} \\
% Show clock-generation schematic (or ideal source representation) used in baseline simulations.
% Indicate period, duty cycle, and any derived phase signals (e.g., SE, precharge).



\newpage

\subsubsection{Tier 5: Full Array Integration}

\noindent\underline{16$\times$4 Array (Hierarchical)} \\
% Show hierarchical schematic instantiating 16 wordlines by 4 bitlines of 1T1C cells.
% Include connections to WL drivers, bitlines, sense amps, precharge circuits, and decoders.



\newpage

\noindent\underline{Top-Level DRAM Module} \\
% Show final top-level block exposing address, data in/out, WE, CLK, and any control/status pins.
% Annotate how lower-tier blocks are interconnected and where timing-critical paths originate.



\newpage

\subsection{Design Strategy}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{writeup/DRAM_VARIANTS.png}
  \caption{DRAM bitcell topologies considered in this project: (a) 1T1C, (b) 2T1C, and (c) 3T1C.}
  \label{fig:dram_cells}
\end{figure}

\noindent Our starting point for this project was the decision to focus on DRAM rather than SRAM. The basic version of the course project uses a 6T SRAM macro, but we explicitly chose to implement a DRAM style macro in order to engage with charge based storage, refresh, and bitline sensing. Once we committed to DRAM, the main design choice became the bitcell topology. Figure~\ref{fig:dram_cells} compares the three options we considered: a conventional 1T1C cell, a 2T1C cell with separate write and read access devices, and a 3T1C cell that further decouples the read path. All three cells store data as charge on a capacitor tied to ground, but they differ in how that storage node is connected to the wordlines and bitlines.

\vspace{5mm}

\noindent In the 1T1C cell in Figure~\ref{fig:dram_cells}(a), a single access transistor connects the storage capacitor to the bitline BL under control of a single wordline WL. During precharge, BL is driven to approximately $V_{\mathrm{DD}}/2$ and WL is low, so the capacitor is isolated. During an activate, WL is raised, the access device turns on, and the small charge on the capacitor shares with the large bitline capacitance. This produces only a small voltage perturbation around $V_{\mathrm{DD}}/2$ on BL, which must then be detected and amplified by a sensitive sense amplifier before being restored back into the capacitor. The advantages of this topology are density and simplicity: one transistor and one capacitor per bit give the minimum cell area and the most regular layout. The disadvantages, especially in a small on chip macro, are that read and write both directly disturb the storage node, sense margins are tight because the bitline swing is small, and the cell has no protection against read disturb or leakage beyond periodic refresh.

\vspace{5mm}

\noindent The 2T1C cell in Figure~\ref{fig:dram_cells}(b) adds a second access transistor so that reads and writes can use distinct wordlines and bitlines. A typical organization uses a write bitline WBL and write wordline WWL to connect to the storage node SN during writes, and a separate read transistor controlled by a read wordline RWL that connects SN to a read bitline RBL during reads. The storage mechanism is still purely dynamic: the capacitor C at SN is charged or discharged during a write, then slowly leaks; a read connects SN to a precharged RBL so that the small voltage change can be sensed. Compared to 1T1C, the extra device buys flexibility. The write path can be optimized for strong drive and full swing updates of the capacitor, while the read path can be optimized to minimize disturb, for example by only allowing a small amount of charge to flow onto RBL and by timing RWL so that the storage node is not heavily loaded. This relaxes sense amplifier requirements and helps with read stability, at the cost of roughly a doubling of transistor count per bit and a more complex routing of WBL, RBL, WWL, and RWL across the array.

\vspace{5mm}

\noindent The 3T1C cell in Figure~\ref{fig:dram_cells}(c), which we ultimately adopted, extends this separation further by interposing an additional transistor between the storage node and the read bitline. In our implementation, a write transistor controlled by a write wordline WWL connects the write bitline WBL to the capacitor node, allowing robust full swing writes. The capacitor node then drives an internal node $V_{\mathrm{c}}$, which is connected through a dedicated read gate to the read bitline RBL under control of a read wordline RWL. This extra degree of freedom lets us bias and time WWL and RWL independently: we can use an aggressive voltage swing and longer pulse on WWL during writes to fully charge or discharge the capacitor, while using a shorter, carefully shaped RWL pulse that only weakly couples the storage node into the read path. In effect, the 3T1C topology lets us view the read path as a gain stage that samples the stored voltage onto RBL with reduced disturb to the capacitor. The tradeoff is further increased cell area and layout complexity compared to 1T1C and 2T1C, but in exchange we gain significantly better read margin, lower risk of read disturb, and more flexibility in designing the sense amplifier because the effective signal seen at RBL can be larger and more controlled.

\vspace{5mm}

\noindent In the standard version of this project, each team must implement both a baseline memory macro and a significantly optimized version, and then quantify the improvement in a figure of merit that combines area, energy, and performance. Our group instead chose a more challenging DRAM based design with a 3T1C bitcell as the baseline, and, in consultation with the course instructor and TAs, we were not required to also tape out an optimized circuit level variant. In lieu of a second physical implementation, we use the remainder of this subsection to survey a small subset of modern low level DRAM and embedded DRAM research that targets the same underlying metrics as our project: sense margin and access delay at the bitcell and bitline level, dynamic and static energy in precharge and sensing, and area overhead in the cell and peripheral circuits. The goal is to place our simple 3T1C macro within the broader landscape of DRAM bitcell and sense amplifier design, and to understand what kinds of circuit techniques current research uses to push density, energy, and delay beyond what our straightforward layout can achieve.

\newpage

\subsection{Modern DRAM optimizations and literature review}

\noindent In the standard version of this project, each team must implement both a baseline memory macro and a significantly optimized version, and then quantify the improvement in a figure of merit that combines area, energy, and performance. Our group instead chose a more challenging DRAM-based design with a 3T1C bitcell as the baseline, and, in consultation with the course staff, we were not required to also produce and evaluate a second, optimized macro. In lieu of a second physical implementation, this subsection surveys a small, non-representative subset of modern DRAM research that targets the same metrics as our project (effective access latency, dynamic and static energy, and area overhead), but at the level of bitcell topology and very-low-level peripheral circuitry. DRAM optimization is a very broad field, including topics such as 3D integration, new interfaces, security, near-memory compute, and system-level refresh policies; the works selected here focus narrowly on cell-level gain-cell eDRAM design and sense-amplifier and bitline energy reduction, which are the closest analogues to the choices we make in our 3T1C macro.

\vspace{5mm}

\noindent A useful starting point is the survey and taxonomy of gain-cell eDRAM (GC-eDRAM) structures by Teman et al.\ \cite{Teman2012GCReview}. They classify embedded DRAM bitcells that can be fabricated in standard logic CMOS without deep-trench or stacked capacitors, and contrast them with conventional 1T1C cells. In a 1T1C cell the storage capacitance is provided by a dedicated capacitor device, so density is excellent but the process requires additional capacitor steps and the read operation is destructive, forcing immediate restore. In logic-compatible gain cells, the effective storage capacitance is the parasitic capacitance at an internal storage node, written and read through one or more transistors. Teman et al.\ show that 2T and 3T gain cells generally occupy more area than an optimized custom 1T1C cell, but still achieve substantially higher density than compiled 6T SRAM in the same technology, while offering non-destructive read and the ability to decouple write and read ports. The survey also emphasizes how data retention is limited by subthreshold, gate, and junction leakage at the storage node, and how device choices (high-$V_{\text{T}}$ versus regular-$V_{\text{T}}$), biasing of the wordlines during hold, and boosting or undervolting of write lines all trade off retention time against write margin and peripheral complexity.

\vspace{5mm}

\noindent Giterman et al.\ push this line of work into a modern FinFET process with a 1~Mbit 3T gain-cell eDRAM implemented in a 16~nm logic-compatible FinFET technology \cite{Giterman2020GC3T}. Their bitcell uses three transistors with carefully chosen threshold voltages to balance leakage paths to and from the storage node. The implemented 1~Mbit array is fully logic-compatible and provides a bitcell size that is approximately 2$\times$ smaller than a 6T SRAM cell drawn with the same design rules, while keeping the array layout on standard logic tracks \cite{Giterman2020GC3T}. Measurement results demonstrate a data-retention time of about 77~\textmu s under $V_{\mathrm{DD}} = 0.6$~V, which is more than 10$\times$ longer than previously reported GC-eDRAM implementations in 28~nm technologies, and correct operation was demonstrated from $-40^{\circ}\mathrm{C}$ to $125^{\circ}\mathrm{C}$ and down to a minimum supply voltage of roughly 0.45~V \cite{Giterman2020GC3T}. At the macro level, the design uses replica gain cells and self-timed control to adapt the read pulse width to the actual bitline development and employs write drivers that restore the write bitlines after each operation to suppress leakage of stored ``1'' levels. For our 3T1C macro, this paper is a concrete proof that a 3T gain cell can be competitive in density with SRAM in advanced nodes while still achieving tens of microseconds of retention without explicit capacitors, provided that the leakage paths and peripheral timing are co-optimized.

\vspace{5mm}

\noindent Kim and Park take a different approach and explicitly trade additional static circuitry for effectively infinite retention time by turning a 2T gain cell into a pseudo-static embedded DRAM suitable for analog processing-in-memory (PIM) \cite{Kim2022PSGC}. They start from a conventional 2T1C-style gain cell and show, via simulation across technologies from 180~nm to 28~nm, that retention time for a fixed cell structure degrades severely as the channel length shrinks because the storage-node capacitance is reduced and subthreshold leakage grows. They then introduce a pseudo-static gain cell (PS-GC) that combines a 2T storage cell with an active leakage-compensation network that sources or sinks current to hold the storage-node voltage near its ideal level during retention. In a 28~nm CMOS implementation, the resulting bitcell area is reported as 0.79$\times$ that of a compiled 6T SRAM cell and 0.58$\times$ that of an 8T SRAM cell optimized for analog MAC operations, even though the PS-GC uses five transistors, because it avoids a dedicated large metal-oxide-metal capacitor inside the cell \cite{Kim2022PSGC}. Post-layout simulations of a 64$\times$64 macro show that the PS-GC array maintains pseudo-static operation with effectively unlimited retention time over wide process, voltage, and temperature ranges, and that at an operating frequency of 667~MHz it can function from 0.9--1.2~V and $-25^{\circ}\mathrm{C}$ to $85^{\circ}\mathrm{C}$ with both read and write access times below 0.3~ns at 1.2~V and $85^{\circ}\mathrm{C}$ and a static power of about 2.2~nW/bit at 25~\textdegree C \cite{Kim2022PSGC}. Conceptually, this work demonstrates that adding an active leakage-compensation path can stretch retention essentially to pseudo-infinite, turning DRAM into SRAM-like storage without an explicit storage capacitor, at the cost of additional devices and static bias currents. Our 3T1C cell does not implement the full PS-GC assist circuitry, but the same idea of explicitly engineering all leakage paths around the storage node appears in our device choices and biasing schemes for the write wordline.

\vspace{5mm}

\noindent The previous works primarily manipulate the bitcell itself. A complementary axis of low-level DRAM optimization lies in the sensing and precharge circuitry, where every access incurs energy to precharge the bitlines to approximately $V_{\mathrm{DD}}/2$, to sense the small voltage perturbation caused by charge sharing with the cell capacitance, and then to restore both the cell and the bitline. Dai et al.\ analyze this energy flow and propose single-bitline-load sense-amplifier (SBLSA) circuits that reduce bitline and sense-amplifier energy in a DRAM array by avoiding unnecessary full-swing bitline transitions \cite{Dai2023SBLSA}. Building on an earlier single-bitline-write (SBW) scheme, they introduce two SBLSA variants: a redundant-voltage-discharged SBLSA (RVD-SBLSA) that uses an extra discharge path to clamp the target bitline back toward $V_{\mathrm{DD}}/2$ after a read or write, and a bit-aware SBLSA (BA-SBLSA) that conditionally enables single- or dual-bitline loading based on the stored data pattern \cite{Dai2023SBLSA}. Circuit-level simulations in a 65~nm CMOS process show that both variants reduce total read and write energy relative to a conventional differential sense amplifier across data patterns, with the RVD-SBLSA giving the lowest energy and the BA-SBLSA offering a more balanced energy–latency trade-off \cite{Dai2023SBLSA}. Although these designs still assume a standard 1T1C cell, the underlying principle is directly relevant to our work: a significant fraction of per-access energy is tied up in the large bitline capacitances and in the precise sequence of precharge, sense, and restore events, so small changes to the sense-amplifier topology and timing can yield meaningful energy savings even when the bitcell is fixed.

\vspace{5mm}

\noindent Finally, it is useful to place these gain-cell designs in the broader context of contemporary embedded DRAM implementations that still use conventional 1T1C cells. For example, Lin et al.\ report a 14~nm SOI FinFET CMOS technology that integrates 0.0174~\textmu m$^{2}$ 1T1C eDRAM cells alongside logic \cite{Lin2014eDRAM14nm}, and Hamzaoglu et al.\ demonstrate a 1~Gb, multi-gigahertz embedded DRAM macro in a 22~nm tri-gate CMOS technology \cite{Hamzaoglu2014eDRAM1Gb}. These macros exploit aggressive capacitor and trench scaling, deep circuit-level optimization of the sense amplifiers and reference schemes, and carefully tuned refresh to meet high-speed cache requirements. Compared to such state-of-the-art 1T1C macros, gain-cell eDRAM like the 3T structures discussed above gives up some areal density in exchange for logic compatibility (no special capacitor steps), non-destructive reads, and additional ports or assist circuits that can be tailored to specific SoC blocks (for example, analog PIM engines). Our choice of a 3T1C cell in this project follows this trend: we accept more transistors per bitcell than a pure 1T1C array would require, but we gain better read stability, a clean separation of write and read paths, and the ability to tune leakage and timing at the circuit level using only the devices provided by a standard CMOS process. The literature reviewed here shows that, in modern technologies, such gain-cell DRAMs can reach retention times in the tens of microseconds with modest area overhead and can be paired with carefully engineered sense-amplifier and bitline circuits to significantly reduce per-access energy, directly aligning with the area–energy–delay trade-offs that define our project’s figure of merit.

\newpage

% Include all block-level schematics and test schematics

% -----------------------------------------
\section{Reasonable Size}

\subsection{Memory Cell Area}
% 3.1



\newpage

\subsection{Other Area Considerations}
% 3.2



\newpage

\subsection{Description of Sizing Rationale with Support}
% 3.3



\newpage

% -----------------------------------------
\section{Design Functionality}

\subsection{Memory Design Validation Description}
% 4.1



\newpage

\subsection{Read Operation Demonstration and Explanation}
% 4.2



\newpage

\subsection{Read Works}
% 4.3



\newpage

\subsection{Write Operation Demonstration and Explanation}
% 4.4



\newpage

\subsection{Write Works}
% 4.5



\newpage

\subsection{Simulated Read/Write to Different Addresses}
% 4.6



\newpage

\subsection{Simulated Read/Write to Same Address}
% 4.7



\newpage

\subsection{Clean, Robust Design}
% 4.8



\newpage

% -----------------------------------------
\section{Reasonable Performance}

\subsection{Precharge Bitlines}
% 5.1



\newpage

\subsection{Write Driver Driving Bitlines}
% 5.2



\newpage

\subsection{Address to Wordline Select}
% 5.3



\newpage

\subsection{Cell Driving Bitline During Read}
% 5.4



\newpage

% -----------------------------------------
\section{Design Metrics with Support}
% 6



\newpage

% FOM, delay, power, area evidence

\begin{thebibliography}{99}

\bibitem{Teman2012GCReview}
A.~Teman, P.~Meinerzhagen, A.~Burg, and A.~Fish,
``Review and classification of gain cell eDRAM implementations,''
in \textit{Proc.\ IEEE 27th Convention of Electrical and Electronics Engineers in Israel (IEEEI)}, Eilat, Israel, Nov.\ 2012, pp.~1--5.

\bibitem{Giterman2020GC3T}
R.~Giterman, A.~Shalom, A.~Burg, A.~Fish, and A.~Teman,
``A 1-Mbit fully logic-compatible 3T gain-cell embedded DRAM in 16-nm FinFET,''
\textit{IEEE Solid-State Circuits Letters}, vol.~3, pp.~110--113, 2020, doi:10.1109/LSSC.2020.3006496.

\bibitem{Kim2022PSGC}
S.~Kim and J.-E.~Park,
``Pseudo-static gain cell of embedded DRAM for processing-in-memory in intelligent IoT sensor nodes,''
\textit{Sensors}, vol.~22, no.~11, Art.~4284, Jun.\ 2022, doi:10.3390/s22114284.

\bibitem{Dai2023SBLSA}
C.~Dai, Y.~Lu, W.~Lu, Z.~Lin, X.~Wu, and C.~Peng,
``Low-power single bitline load sense amplifier for DRAM,''
\textit{Electronics}, vol.~12, no.~19, Art.~4024, Sep.\ 2023, doi:10.3390/electronics12194024.

\bibitem{Lin2014eDRAM14nm}
C.-H.~Lin \textit{et al.},
``High-performance 14 nm SOI FinFET CMOS technology with 0.0174 $\mu\text{m}^{2}$ embedded DRAM and 15 levels of Cu metallization,''
in \textit{Proc.\ IEEE Int.\ Electron Devices Meeting (IEDM)}, San Francisco, CA, USA, Dec.\ 2014, pp.~3.1.1--3.1.4.

\bibitem{Hamzaoglu2014eDRAM1Gb}
F.~Hamzaoglu \textit{et al.},
``A 1 Gb 2 GHz embedded DRAM in 22 nm tri-gate CMOS technology,''
in \textit{IEEE Int.\ Solid-State Circuits Conf.\ (ISSCC) Dig.\ Tech.\ Papers}, San Francisco, CA, USA, Feb.\ 2014, pp.~324--325.

\end{thebibliography}

\end{document}
